{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quick Guide: Exploring Hugging Face Datasets\n",
        "\n",
        "A simple walkthrough of loading and inspecting datasets before fine-tuning.\n",
        "\n",
        "**Goal**: Before we fine-tune a model, we need to understand the dataset structure and content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/ca/51/409a8184ed35453d9cbb3d6b20d524b1115c2c2d117b85d5e9b06cd70b45/datasets-4.3.0-py3-none-any.whl.metadata\n",
            "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: filelock in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from datasets) (2.3.4)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Obtaining dependency information for pyarrow>=21.0.0 from https://files.pythonhosted.org/packages/af/63/ba23862d69652f85b615ca14ad14f3bcfc5bf1b99ef3f0cd04ff93fdad5a/pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
            "  Obtaining dependency information for dill<0.4.1,>=0.3.0 from https://files.pythonhosted.org/packages/50/3d/9373ad9c56321fdab5b41197068e1d8c25883b3fea29dd361f9b55116869/dill-0.4.0-py3-none-any.whl.metadata\n",
            "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
            "Collecting httpx<1.0.0 (from datasets)\n",
            "  Obtaining dependency information for httpx<1.0.0 from https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl.metadata\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/79/35/0429ee11d035fc33abe32dca1b2b69e8c18d236547b9a9b72c1929189b9a/xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
            "  Using cached xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Obtaining dependency information for multiprocess<0.70.17 from https://files.pythonhosted.org/packages/0a/7d/a988f258104dcd2ccf1ed40fdc97e26c4ac351eeaf81d76e266c52d84e2f/multiprocess-0.70.16-py312-none-any.whl.metadata\n",
            "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec[http]<=2025.9.0,>=2023.1.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from datasets) (2025.9.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Obtaining dependency information for aiohttp!=4.0.0a0,!=4.0.0a1 from https://files.pythonhosted.org/packages/28/66/d35dcfea8050e131cdd731dff36434390479b4045a8d0b9d7111b0a968f1/aiohttp-3.13.2-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading aiohttp-3.13.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
            "Collecting anyio (from httpx<1.0.0->datasets)\n",
            "  Obtaining dependency information for anyio from https://files.pythonhosted.org/packages/15/b3/9b1a8074496371342ec1e796a96f99c82c945a339cd81a8e73de28b4cf9e/anyio-4.11.0-py3-none-any.whl.metadata\n",
            "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: certifi in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
            "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl.metadata\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
            "  Obtaining dependency information for h11>=0.16 from https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl.metadata\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Obtaining dependency information for aiohappyeyeballs>=2.5.0 from https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Obtaining dependency information for aiosignal>=1.4.0 from https://files.pythonhosted.org/packages/fb/76/641ae371508676492379f16e2fa48f4e2c11741bd63c48be4b12a6b09cba/aiosignal-1.4.0-py3-none-any.whl.metadata\n",
            "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Obtaining dependency information for attrs>=17.3.0 from https://files.pythonhosted.org/packages/3a/2a/7cc015f5b9f5db42b7d48157e23356022889fc354a2813c15934b7cb5c0e/attrs-25.4.0-py3-none-any.whl.metadata\n",
            "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/2b/94/5c8a2b50a496b11dd519f4a24cb5496cf125681dd99e94c604ccdea9419a/frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
            "  Using cached frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (20 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Obtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/b3/93/c4f67a436dd026f2e780c433277fff72be79152894d9fc36f44569cab1a6/multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
            "  Using cached multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Obtaining dependency information for propcache>=0.2.0 from https://files.pythonhosted.org/packages/0a/b6/5c9a0e42df4d00bfb4a3cbbe5cf9f54260300c88a0e9af1f47ca5ce17ac0/propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
            "  Using cached propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Obtaining dependency information for yarl<2.0,>=1.17.0 from https://files.pythonhosted.org/packages/ba/f5/0601483296f09c3c65e303d60c070a5c19fcdbc72daa061e96170785bc7d/yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
            "  Using cached yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (75 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets)\n",
            "  Obtaining dependency information for sniffio>=1.1 from https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "Downloading pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl (34.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
            "Downloading aiohttp-3.13.2-cp312-cp312-macosx_11_0_arm64.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.8/491.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
            "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Using cached frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl (50 kB)\n",
            "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Using cached multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
            "Using cached propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl (47 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl (94 kB)\n",
            "Installing collected packages: xxhash, sniffio, pyarrow, propcache, multidict, h11, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, httpcore, anyio, aiosignal, httpx, aiohttp, datasets\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.11.0 attrs-25.4.0 datasets-4.3.0 dill-0.4.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 multidict-6.7.0 multiprocess-0.70.16 propcache-0.4.1 pyarrow-22.0.0 sniffio-1.3.1 xxhash-3.6.0 yarl-1.22.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load the Dataset\n",
        "\n",
        "We'll use the famous **Alpaca dataset** - a collection of instruction-following examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Loading the Alpaca Dataset from Hugging Face\n",
            "======================================================================\n",
            "\n",
            "Dataset loaded! Let's explore it...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"Loading the Alpaca Dataset from Hugging Face\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
        "\n",
        "print(\"Dataset loaded! Let's explore it...\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Check Available Splits\n",
        "\n",
        "Most datasets have train/validation/test splits. Let's see what we have:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available splits:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['instruction', 'input', 'output', 'text'],\n",
            "        num_rows: 52002\n",
            "    })\n",
            "})\n",
            "\n",
            "Number of examples: 52,002\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Available splits:\")\n",
        "print(dataset)\n",
        "print()\n",
        "print(f\"Number of examples: {len(dataset['train']):,}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Understand the Structure\n",
        "\n",
        "What fields does each example have?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Dataset Structure\n",
            "======================================================================\n",
            "\n",
            "Fields in each example:\n",
            "['instruction', 'input', 'output', 'text']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"Dataset Structure\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "print(\"Fields in each example:\")\n",
        "print(dataset['train'].column_names)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Look at Examples\n",
        "\n",
        "Let's examine some actual examples to understand the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First example:\n",
            "----------------------------------------------------------------------\n",
            "instruction: Give three tips for staying healthy.\n",
            "input: \n",
            "output: 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule.\n",
            "text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Give three tips for staying healthy.\n",
            "\n",
            "### Response:\n",
            "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"First example:\")\n",
        "print(\"-\" * 70)\n",
        "first = dataset['train'][0]\n",
        "for key, value in first.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------\n",
            "Random example:\n",
            "----------------------------------------------------------------------\n",
            "instruction: Update the following sentence with the right punctuation\n",
            "input: What are you doing\n",
            "output: What are you doing?\n",
            "text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Update the following sentence with the right punctuation\n",
            "\n",
            "### Input:\n",
            "What are you doing\n",
            "\n",
            "### Response:\n",
            "What are you doing?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"-\" * 70)\n",
        "print(\"Random example:\")\n",
        "print(\"-\" * 70)\n",
        "random_ex = dataset['train'][random.randint(0, len(dataset['train']) - 1)]\n",
        "for key, value in random_ex.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Quick Statistics\n",
        "\n",
        "Let's gather some useful statistics about the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Quick Statistics\n",
            "======================================================================\n",
            "\n",
            "Output length - Min: 0, Max: 4181, Avg: 270 chars\n",
            "Examples with empty input field: 31,323 (60.2%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"Quick Statistics\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "# Check output lengths\n",
        "output_lengths = [len(ex['output']) for ex in dataset['train']]\n",
        "print(f\"Output length - Min: {min(output_lengths)}, Max: {max(output_lengths)}, Avg: {sum(output_lengths)/len(output_lengths):.0f} chars\")\n",
        "\n",
        "# Check for empty inputs\n",
        "empty_inputs = sum(1 for ex in dataset['train'] if not ex['input'].strip())\n",
        "print(f\"Examples with empty input field: {empty_inputs:,} ({100*empty_inputs/len(dataset['train']):.1f}%)\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Create Train/Validation Split\n",
        "\n",
        "The Alpaca dataset only has a train split. Let's create a validation set:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Creating Train/Validation Split\n",
            "======================================================================\n",
            "\n",
            "Training examples: 46,801\n",
            "Validation examples: 5,201\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"Creating Train/Validation Split\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "print(f\"Training examples: {len(split_dataset['train']):,}\")\n",
        "print(f\"Validation examples: {len(split_dataset['test']):,}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Loading is easy**: Use `load_dataset(\"dataset-name\")` from Hugging Face\n",
        "2. **Inspect the structure**: Check column names and splits before training\n",
        "3. **Sample examples**: Always look at actual examples to understand the data format\n",
        "4. **Statistics matter**: Check for empty fields, length distributions, etc.\n",
        "5. **Create validation sets**: If the dataset doesn't have one, split it yourself\n",
        "\n",
        "**Next Steps**: \n",
        "- Format these examples for instruction tuning\n",
        "- Tokenize the data\n",
        "- Create a DataLoader for training\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
