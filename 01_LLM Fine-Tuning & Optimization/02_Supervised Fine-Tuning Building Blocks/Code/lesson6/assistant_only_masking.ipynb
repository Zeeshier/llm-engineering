{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0c9dcc",
   "metadata": {},
   "source": [
    "# Assistant-Only Masking (Instruction Masking)\n",
    "\n",
    "This notebook demonstrates how to mask user prompts during training so the model only learns from assistant responses.\n",
    "\n",
    "**Key Concept**: When fine-tuning instruction models, we set labels to `-100` for user input tokens so the model only computes loss on the assistant's responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa887e3",
   "metadata": {},
   "source": [
    "![assistant-only-masking.png](assistant-only-masking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85601951",
   "metadata": {},
   "source": [
    "## Step 1: Load Tokenizer\n",
    "\n",
    "We'll use Llama 3.2 1B Instruct, which has special tokens for formatting conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4655a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c7d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a popular model's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6096d3",
   "metadata": {},
   "source": [
    "## Step 2: Define a Conversation\n",
    "\n",
    "A simple user-assistant exchange:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2a3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The answer is 4.\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca0f24",
   "metadata": {},
   "source": [
    "## Step 3: Apply Chat Template and Get Input IDs\n",
    "\n",
    "The `apply_chat_template()` method formats the conversation with special tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0f41b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1682, 5020, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 3923, 374, 220, 17, 10, 17, 30, 128009, 128006, 78191, 128007, 271, 791, 4320, 374, 220, 19, 13, 128009]\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 29 Oct 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 2+2?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The answer is 4.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Get input_ids\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=False\n",
    ")\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "\n",
    "print(input_ids)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99856e4",
   "metadata": {},
   "source": [
    "## Step 4: Find Where Assistant Response Starts\n",
    "\n",
    "We need to identify where the assistant's response begins by finding the assistant header tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3603dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant_header_ids=[128006, 78191, 128007, 271]\n",
      "128006 <|start_header_id|>\n",
      "78191 assistant\n",
      "128007 <|end_header_id|>\n",
      "271 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find where assistant response starts by looking for the assistant header token\n",
    "# For Llama 3, assistant header is: <|start_header_id|>assistant<|end_header_id|>\\n\\n\n",
    "assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "assistant_header_ids = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "\n",
    "print(f\"assistant_header_ids={assistant_header_ids}\")\n",
    "\n",
    "for token in assistant_header_ids:\n",
    "    print(token, tokenizer.decode(token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db10670",
   "metadata": {},
   "source": [
    "### Find the Assistant Start Position\n",
    "\n",
    "Search through input_ids to find where the assistant header appears:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa471f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx of assistant_start = 42\n"
     ]
    }
   ],
   "source": [
    "# Find where assistant header appears in input_ids\n",
    "assistant_start = None\n",
    "for i in range(len(input_ids) - len(assistant_header_ids)):\n",
    "    if input_ids[i : i + len(assistant_header_ids)] == assistant_header_ids:\n",
    "        assistant_start = i + len(assistant_header_ids)  # Start after the header\n",
    "        break\n",
    "print(f\"Idx of assistant_start = {assistant_start}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b7377",
   "metadata": {},
   "source": [
    "## Step 5: Create Labels with Masking\n",
    "\n",
    "Create labels array:\n",
    "- Set `-100` for all user tokens (these will be ignored in loss calculation)\n",
    "- Copy input_ids for assistant tokens (these will be used for learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af2e619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 791, 4320, 374, 220, 19, 13, 128009]\n"
     ]
    }
   ],
   "source": [
    "# Create labels\n",
    "labels = [-100] * len(input_ids)  # Mask everything by default\n",
    "if assistant_start is not None:\n",
    "    # Unmask from assistant response onwards\n",
    "    labels[assistant_start:] = input_ids[assistant_start:]\n",
    "\n",
    "print(len(labels), labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5961c",
   "metadata": {},
   "source": [
    "## Step 6: Visualize the Masking\n",
    "\n",
    "Let's see which tokens are masked and which are used for learning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbc787cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MASKING VISUALIZATION\n",
      "======================================================================\n",
      "Pos   Token                     Input ID     Label        Status    \n",
      "----------------------------------------------------------------------\n",
      "0     <|begin_of_text|>         128000       -100         MASKED    \n",
      "1     <|start_header_id|>       128006       -100         MASKED    \n",
      "2     system                    9125         -100         MASKED    \n",
      "3     <|end_header_id|>         128007       -100         MASKED    \n",
      "4     \\n\\n                      271          -100         MASKED    \n",
      "5     Cut                       38766        -100         MASKED    \n",
      "6     ting                      1303         -100         MASKED    \n",
      "7      Knowledge                33025        -100         MASKED    \n",
      "8      Date                     2696         -100         MASKED    \n",
      "9     :                         25           -100         MASKED    \n",
      "10     December                 6790         -100         MASKED    \n",
      "11                              220          -100         MASKED    \n",
      "12    202                       2366         -100         MASKED    \n",
      "13    3                         18           -100         MASKED    \n",
      "14    \\n                        198          -100         MASKED    \n",
      "15    Today                     15724        -100         MASKED    \n",
      "16     Date                     2696         -100         MASKED    \n",
      "17    :                         25           -100         MASKED    \n",
      "18                              220          -100         MASKED    \n",
      "19    29                        1682         -100         MASKED    \n",
      "20     Oct                      5020         -100         MASKED    \n",
      "21                              220          -100         MASKED    \n",
      "22    202                       2366         -100         MASKED    \n",
      "23    5                         20           -100         MASKED    \n",
      "24    \\n\\n                      271          -100         MASKED    \n",
      "25    <|eot_id|>                128009       -100         MASKED    \n",
      "26    <|start_header_id|>       128006       -100         MASKED    \n",
      "27    user                      882          -100         MASKED    \n",
      "28    <|end_header_id|>         128007       -100         MASKED    \n",
      "29    \\n\\n                      271          -100         MASKED    \n",
      "30    What                      3923         -100         MASKED    \n",
      "31     is                       374          -100         MASKED    \n",
      "32                              220          -100         MASKED    \n",
      "33    2                         17           -100         MASKED    \n",
      "34    +                         10           -100         MASKED    \n",
      "35    2                         17           -100         MASKED    \n",
      "36    ?                         30           -100         MASKED    \n",
      "37    <|eot_id|>                128009       -100         MASKED    \n",
      "38    <|start_header_id|>       128006       -100         MASKED    \n",
      "39    assistant                 78191        -100         MASKED    \n",
      "40    <|end_header_id|>         128007       -100         MASKED    \n",
      "41    \\n\\n                      271          -100         MASKED    \n",
      "42    The                       791          791          LEARNS    \n",
      "43     answer                   4320         4320         LEARNS    \n",
      "44     is                       374          374          LEARNS    \n",
      "45                              220          220          LEARNS    \n",
      "46    4                         19           19           LEARNS    \n",
      "47    .                         13           13           LEARNS    \n",
      "48    <|eot_id|>                128009       128009       LEARNS    \n"
     ]
    }
   ],
   "source": [
    "# Visualize\n",
    "print(\"=\" * 70)\n",
    "print(\"MASKING VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Pos':<5} {'Token':<25} {'Input ID':<12} {'Label':<12} {'Status':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, (inp, lab) in enumerate(zip(input_ids, labels)):\n",
    "    token = tokenizer.decode([inp]).replace(\"\\n\", \"\\\\n\")\n",
    "    status = \"LEARNS\" if lab != -100 else \"MASKED\"\n",
    "    print(f\"{i:<5} {token:<25} {inp:<12} {lab:<12} {status:<10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73f0b1",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "How many tokens are masked vs. used for learning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1429d6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Total tokens: 49\n",
      "Masked (user): 42\n",
      "Learning (assistant): 7\n",
      "Learning ratio: 14.3%\n"
     ]
    }
   ],
   "source": [
    "# Summary stats\n",
    "masked_count = sum(1 for x in labels if x == -100)\n",
    "learn_count = len(labels) - masked_count\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Total tokens: {len(labels)}\")\n",
    "print(f\"Masked (user): {masked_count}\")\n",
    "print(f\"Learning (assistant): {learn_count}\")\n",
    "print(f\"Learning ratio: {learn_count/len(labels)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2688084",
   "metadata": {},
   "source": [
    "## PyTorch Ignores -100 Labels\n",
    "\n",
    "Let's verify that PyTorch's CrossEntropyLoss actually ignores positions with label `-100`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18c6aa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with all valid labels: 0.5743\n",
      "Loss with first two masked:  0.8388\n",
      "Loss with only third token:  0.8388\n",
      "\n",
      "✓ Loss2 (0.8388) == Loss3 (0.8388): True\n",
      "  → PyTorch ignores -100 labels!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simple example: 3 predictions, vocab size = 5\n",
    "logits = torch.tensor([[1.0, 2.0, 0.5, 0.1, 0.3],\n",
    "                       [0.2, 0.1, 3.0, 0.5, 0.2],\n",
    "                       [2.0, 0.5, 0.3, 1.5, 0.8]])\n",
    "\n",
    "# Case 1: All labels are valid\n",
    "labels1 = torch.tensor([1, 2, 0])\n",
    "loss1 = F.cross_entropy(logits, labels1)\n",
    "print(f\"Loss with all valid labels: {loss1:.4f}\")\n",
    "\n",
    "# Case 2: First two labels are -100 (masked)\n",
    "labels2 = torch.tensor([-100, -100, 0])\n",
    "loss2 = F.cross_entropy(logits, labels2)\n",
    "print(f\"Loss with first two masked:  {loss2:.4f}\")\n",
    "\n",
    "# Case 3: Only the third position (label=0)\n",
    "loss3 = F.cross_entropy(logits[2:3], labels1[2:3])\n",
    "print(f\"Loss with only third token:  {loss3:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Loss2 ({loss2:.4f}) == Loss3 ({loss3:.4f}): {torch.allclose(loss2, loss3)}\")\n",
    "print(\"  → PyTorch ignores -100 labels!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e611332",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **`-100` is the magic number**: PyTorch's CrossEntropyLoss ignores any label set to `-100`\n",
    "2. **Mask user tokens**: Set labels to `-100` for all user input and special tokens\n",
    "3. **Keep assistant tokens**: Copy input_ids to labels for assistant responses\n",
    "4. **Why this matters**: The model only learns to generate assistant responses, not to mimic user inputs\n",
    "\n",
    "This technique is essential for instruction tuning and chat model fine-tuning!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
