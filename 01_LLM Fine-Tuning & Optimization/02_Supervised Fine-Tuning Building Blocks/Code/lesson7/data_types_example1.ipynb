{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Data Types: FP32, FP16, and BF16\n",
        "\n",
        "This notebook demonstrates the memory differences between different floating-point data types used in deep learning.\n",
        "\n",
        "**Key Concept**: Using lower precision data types (FP16, BF16) can significantly reduce memory usage compared to FP32, enabling larger models and batch sizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![dtypes.png](dtypes.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Define Tensor Dimensions\n",
        "\n",
        "We'll create a large tensor to see the memory impact clearly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor shape: 100,000 x 1,000\n",
            "Total elements: 100,000,000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define tensor dimensions\n",
        "import torch\n",
        "\n",
        "rows = 100_000\n",
        "cols = 1_000\n",
        "total_elements = rows * cols\n",
        "\n",
        "print(f\"Tensor shape: {rows:,} x {cols:,}\")\n",
        "print(f\"Total elements: {total_elements:,}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## FP32 (Float32) - The Baseline\n",
        "\n",
        "FP32 is the standard 32-bit floating point format. Each element uses **4 bytes** of memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FP32 (Float32)\n",
            "============================================================\n",
            "Bytes per element: 4\n",
            "Total size: 400,000,000 bytes\n",
            "Total size: 381.47 MB\n",
            "\n",
            "Manual calculation: 100,000,000 elements × 4 bytes = 400,000,000 bytes\n",
            "Manual calculation: 381.47 MB\n",
            "Verification: ✓ Match!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create FP32 tensor\n",
        "print(\"=\" * 60)\n",
        "print(\"FP32 (Float32)\")\n",
        "print(\"=\" * 60)\n",
        "tensor_fp32 = torch.randn(rows, cols, dtype=torch.float32)\n",
        "size_fp32_bytes = tensor_fp32.element_size() * tensor_fp32.nelement()\n",
        "size_fp32_mb = size_fp32_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"Bytes per element: {tensor_fp32.element_size()}\")\n",
        "print(f\"Total size: {size_fp32_bytes:,} bytes\")\n",
        "print(f\"Total size: {size_fp32_mb:.2f} MB\")\n",
        "\n",
        "# Manual calculation verification\n",
        "manual_size_bytes = total_elements * 4  # 4 bytes per fp32\n",
        "manual_size_mb = manual_size_bytes / (1024 ** 2)\n",
        "print(f\"\\nManual calculation: {total_elements:,} elements × 4 bytes = {manual_size_bytes:,} bytes\")\n",
        "print(f\"Manual calculation: {manual_size_mb:.2f} MB\")\n",
        "print(f\"Verification: {'✓ Match!' if size_fp32_bytes == manual_size_bytes else '✗ Mismatch'}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## FP16 (Float16) - Half Precision\n",
        "\n",
        "FP16 uses only **2 bytes** per element - half the memory of FP32!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FP16 (Float16)\n",
            "============================================================\n",
            "Bytes per element: 2\n",
            "Total size: 200,000,000 bytes\n",
            "Total size: 190.73 MB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create FP16 tensor\n",
        "print(\"=\" * 60)\n",
        "print(\"FP16 (Float16)\")\n",
        "print(\"=\" * 60)\n",
        "tensor_fp16 = torch.randn(rows, cols, dtype=torch.float16)\n",
        "size_fp16_bytes = tensor_fp16.element_size() * tensor_fp16.nelement()\n",
        "size_fp16_mb = size_fp16_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"Bytes per element: {tensor_fp16.element_size()}\")\n",
        "print(f\"Total size: {size_fp16_bytes:,} bytes\")\n",
        "print(f\"Total size: {size_fp16_mb:.2f} MB\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FP16 Range is (-65504, 65504)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65504.0\n",
            "65504.0\n"
          ]
        }
      ],
      "source": [
        "print(torch.tensor(65504.0, dtype=torch.float16).item())\n",
        "print(torch.tensor(65505.0, dtype=torch.float16).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## BF16 (BFloat16) - Brain Float\n",
        "\n",
        "BF16 also uses **2 bytes** per element, but with a different precision trade-off than FP16. It's particularly popular for training LLMs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "BF16 (BFloat16)\n",
            "============================================================\n",
            "Bytes per element: 2\n",
            "Total size: 200,000,000 bytes\n",
            "Total size: 190.73 MB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create BF16 tensor (from FP32)\n",
        "print(\"=\" * 60)\n",
        "print(\"BF16 (BFloat16)\")\n",
        "print(\"=\" * 60)\n",
        "tensor_bf16 = tensor_fp32.to(dtype=torch.bfloat16)\n",
        "size_bf16_bytes = tensor_bf16.element_size() * tensor_bf16.nelement()\n",
        "size_bf16_mb = size_bf16_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"Bytes per element: {tensor_bf16.element_size()}\")\n",
        "print(f\"Total size: {size_bf16_bytes:,} bytes\")\n",
        "print(f\"Total size: {size_bf16_mb:.2f} MB\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: Memory Savings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SUMMARY\n",
            "============================================================\n",
            "FP32: 381.47 MB (baseline)\n",
            "FP16: 190.73 MB (2.0x smaller)\n",
            "BF16: 190.73 MB (2.0x smaller)\n"
          ]
        }
      ],
      "source": [
        "# Summary comparison\n",
        "print(\"=\" * 60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"FP32: {size_fp32_mb:.2f} MB (baseline)\")\n",
        "print(f\"FP16: {size_fp16_mb:.2f} MB ({size_fp32_mb/size_fp16_mb:.1f}x smaller)\")\n",
        "print(f\"BF16: {size_bf16_mb:.2f} MB ({size_fp32_mb/size_bf16_mb:.1f}x smaller)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## FP16 vs BF16: Precision Trade-off\n",
        "\n",
        "Both use 2 bytes, but they make different trade-offs:\n",
        "- **FP16**: More precision, smaller range\n",
        "- **BF16**: Less precision, larger range (same as FP32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LARGE NUMBER TEST\n",
            "============================================================\n",
            "Original (FP32): 500000.000000\n",
            "FP16:            inf\n",
            "BF16:            499712.000000\n",
            "\n",
            "============================================================\n",
            "SMALL PRECISE NUMBER TEST\n",
            "============================================================\n",
            "Original (FP32): 0.123456791\n",
            "FP16:            0.123474121  ✓ More precise\n",
            "BF16:            0.123535156  ✗ Less precise\n",
            "\n",
            "============================================================\n",
            "SUMMARY\n",
            "============================================================\n",
            "• FP16: Better for small precise numbers\n",
            "• BF16: Better for large numbers, same range as FP32\n",
            "• BF16 is preferred for LLM training (numerical stability)\n"
          ]
        }
      ],
      "source": [
        "# Test with a large number\n",
        "large_num = 500000.0\n",
        "\n",
        "fp32_large = torch.tensor([large_num], dtype=torch.float32)\n",
        "fp16_large = torch.tensor([large_num], dtype=torch.float16)\n",
        "bf16_large = torch.tensor([large_num], dtype=torch.bfloat16)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LARGE NUMBER TEST\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Original (FP32): {fp32_large.item():.6f}\")\n",
        "print(f\"FP16:            {fp16_large.item():.6f}\")\n",
        "print(f\"BF16:            {bf16_large.item():.6f}\")\n",
        "print()\n",
        "\n",
        "# Test with a small precise number\n",
        "small_num = 0.123456789\n",
        "\n",
        "fp32_small = torch.tensor([small_num], dtype=torch.float32)\n",
        "fp16_small = torch.tensor([small_num], dtype=torch.float16)\n",
        "bf16_small = torch.tensor([small_num], dtype=torch.bfloat16)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SMALL PRECISE NUMBER TEST\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Original (FP32): {fp32_small.item():.9f}\")\n",
        "print(f\"FP16:            {fp16_small.item():.9f}  ✓ More precise\")\n",
        "print(f\"BF16:            {bf16_small.item():.9f}  ✗ Less precise\")\n",
        "print()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(\"• FP16: Better for small precise numbers\")\n",
        "print(\"• BF16: Better for large numbers, same range as FP32\")\n",
        "print(\"• BF16 is preferred for LLM training (numerical stability)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **FP32** (4 bytes): Full precision, standard format\n",
        "2. **FP16** (2 bytes): Half precision, 2x memory savings\n",
        "3. **BF16** (2 bytes): Brain Float, 2x memory savings with better range than FP16\n",
        "\n",
        "**Why this matters for LLMs:**\n",
        "- Using FP16/BF16 can cut model memory requirements in half\n",
        "- Enables training and inference with larger models or bigger batches\n",
        "- BF16 is becoming the standard for LLM training due to its numerical stability\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
