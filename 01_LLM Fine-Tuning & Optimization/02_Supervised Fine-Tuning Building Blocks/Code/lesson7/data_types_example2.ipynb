{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Size Calculations: Memory Requirements by Data Type\n",
        "\n",
        "This notebook shows how different data types affect the memory requirements for LLMs of various sizes.\n",
        "\n",
        "**Key Question**: How much memory do you need to load a 7B parameter model in FP16 vs FP32?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Define Model Sizes and Data Types\n",
        "\n",
        "We'll calculate memory for common LLM sizes:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard model sizes (in billions of parameters)\n",
        "model_sizes = [1, 7, 13, 70]\n",
        "\n",
        "# Data types and their byte sizes\n",
        "data_types = {\n",
        "    \"FP32\": 4,\n",
        "    \"FP16\": 2,\n",
        "    \"BF16\": 2,\n",
        "    \"INT8\": 1,\n",
        "    \"INT4\": 0.5,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_model_memory(num_params_billions, bytes_per_param):\n",
        "    \"\"\"Calculate memory in GB for a model\"\"\"\n",
        "    num_params = num_params_billions * 1e9\n",
        "    bytes_total = num_params * bytes_per_param\n",
        "    gb = bytes_total / (1024 ** 3)\n",
        "    return gb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Memory Requirements for All Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MODEL SIZE CALCULATIONS: Memory Requirements by Data Type\n",
            "================================================================================\n",
            "\n",
            "Model Size      FP32      FP16      BF16     INT8     INT4\n",
            "        1B   3.73 GB   1.86 GB   1.86 GB  0.93 GB  0.47 GB\n",
            "        7B  26.08 GB  13.04 GB  13.04 GB  6.52 GB  3.26 GB\n",
            "       13B  48.43 GB  24.21 GB  24.21 GB 12.11 GB  6.05 GB\n",
            "       70B 260.77 GB 130.39 GB 130.39 GB 65.19 GB 32.60 GB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create results dictionary\n",
        "results = []\n",
        "\n",
        "for num_params_b in model_sizes:\n",
        "    row = {\"Model Size\": f\"{num_params_b}B\"}\n",
        "    for dtype, bytes_size in data_types.items():\n",
        "        memory_gb = calculate_model_memory(num_params_b, bytes_size)\n",
        "        row[dtype] = f\"{memory_gb:.2f} GB\"\n",
        "    results.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Display the table\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL SIZE CALCULATIONS: Memory Requirements by Data Type\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(df.to_string(index=False))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Insights\n",
        "\n",
        "From the table above, we can see:\n",
        "\n",
        "1. **FP32 → FP16/BF16**: Cuts memory in half (4 bytes → 2 bytes)\n",
        "   - A 7B model: 28 GB → 14 GB\n",
        "   - A 70B model: 280 GB → 140 GB\n",
        "\n",
        "2. **FP16 → INT8**: Cuts memory in half again (2 bytes → 1 byte)\n",
        "   - A 7B model: 14 GB → 7 GB\n",
        "   - Enables running larger models on consumer hardware\n",
        "\n",
        "3. **INT4 (4-bit quantization)**: Most aggressive compression\n",
        "   - A 7B model: Only 3.5 GB!\n",
        "   - Makes it possible to run LLMs on laptops and mobile devices\n",
        "\n",
        "**Important Note**: This calculation only accounts for model weights. In practice, you need additional memory for:\n",
        "- Activations during forward pass\n",
        "- Gradients during training\n",
        "- Optimizer states (Adam uses ~2x model size)\n",
        "- KV cache for generation\n",
        "\n",
        "**Rule of thumb**: For inference, multiply model size by 1.2x. For training, multiply by 4-6x depending on optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Calculation Function\n",
        "\n",
        "The formula is simple: **Memory (GB) = Number of Parameters × Bytes per Parameter / (1024³)**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
