{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Learning Signal: How LLMs Actually Improve\n",
        "\n",
        "This notebook demonstrates the core mechanisms behind how language models learn:\n",
        "- **Cross-Entropy Loss**: Measuring prediction quality\n",
        "- **Label Shifting**: How models learn to predict the next token\n",
        "- **Causal Masking**: Preventing models from seeing the future\n",
        "- **Assistant-Only Masking**: Training chat models to respond, not predict user input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# Uncomment this to login to Hugging Face\n",
        "from dotenv import load_dotenv \n",
        "from huggingface_hub import login\n",
        "load_dotenv()\n",
        "login(token=os.getenv(\"HF_TOKEN\"))\n",
        "\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Cross-Entropy Loss - Measuring Prediction Quality\n",
        "\n",
        "Let's create a scenario where we have:\n",
        "- A **question**: \"What is the capital of France?\"\n",
        "- The **true answer** from training data: \"The capital of France is Paris.\"\n",
        "- A **good predicted answer**: High probability on correct tokens\n",
        "- A **bad predicted answer**: Low probability on correct tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CROSS-ENTROPY LOSS EXAMPLE\n",
            "================================================================================\n",
            "Classes: 0=Cat, 1=Dog, 2=Bird\n",
            "\n",
            "Sample 1: True class = 1 (Dog)\n",
            "Sample 2: True class = 0 (Cat)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Hardcoded example with 3 classes and 2 samples\n",
        "print(\"=\"*80)\n",
        "print(\"CROSS-ENTROPY LOSS EXAMPLE\")\n",
        "print(\"=\"*80)\n",
        "print(\"Classes: 0=Cat, 1=Dog, 2=Bird\")\n",
        "print(\"\\nSample 1: True class = 1 (Dog)\")\n",
        "print(\"Sample 2: True class = 0 (Cat)\")\n",
        "\n",
        "# Sample 1: Model predictions (probabilities for each class)\n",
        "sample1_probs = torch.tensor([0.2, 0.7, 0.1])  # High prob for Dog (correct)\n",
        "sample1_true = torch.tensor([1])  # True class is Dog\n",
        "\n",
        "# Sample 2: Model predictions (probabilities for each class) \n",
        "sample2_probs = torch.tensor([0.1, 0.8, 0.1])  # High prob for Dog (incorrect)\n",
        "sample2_true = torch.tensor([0])  # True class is Cat\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Computing Individual Losses\n",
        "\n",
        "Now let's compute the cross-entropy loss for each sample using PyTorch's `F.cross_entropy()` function:\n",
        "- This function takes **logits** (log probabilities) and the **true class** as input\n",
        "- It returns a loss value: **lower loss = better prediction**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 Loss: 0.3567\n",
            "Example 2 Loss: 2.3026\n",
            "Example 1 Logits: tensor([-1.6094, -0.3567, -2.3026])\n",
            "Example 2 Logits: tensor([-2.3026, -0.2231, -2.3026])\n"
          ]
        }
      ],
      "source": [
        "# Convert probabilities to logits (inverse of softmax)\n",
        "sample1_logits = torch.log(sample1_probs)\n",
        "sample2_logits = torch.log(sample2_probs)\n",
        "\n",
        "# Calculate cross-entropy loss for each sample\n",
        "loss1 = F.cross_entropy(sample1_logits.unsqueeze(0), sample1_true).item()\n",
        "loss2 = F.cross_entropy(sample2_logits.unsqueeze(0), sample2_true).item()\n",
        "\n",
        "loss1 = round(loss1, 4)\n",
        "loss2 = round(loss2, 4)\n",
        "\n",
        "print('Example 1 Loss:', loss1)\n",
        "print('Example 2 Loss:', loss2)\n",
        "\n",
        "print('Example 1 Logits:', sample1_logits)\n",
        "print('Example 2 Logits:', sample2_logits)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Label Shifting - How Language Models Actually Learn\n",
        "\n",
        "In language models, there's an important detail: **we shift the labels by one position**.\n",
        "\n",
        "This sounds complicated, but it's actually very simple!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LABEL SHIFTING EXPLAINED\n",
            "================================================================================\n",
            "\n",
            "Original sentence: 'The capital is Paris'\n",
            "Tokens: ['The', 'capital', 'is', 'Paris']\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "At each position, the model sees some tokens and predicts the NEXT token:\n",
            "\n",
            "Position 0: Model sees: ['The']\n",
            "           Model should predict: 'capital'\n",
            "\n",
            "Position 1: Model sees: ['The', 'capital']\n",
            "           Model should predict: 'is'\n",
            "\n",
            "Position 2: Model sees: ['The', 'capital', 'is']\n",
            "           Model should predict: 'Paris'\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "This is why we SHIFT the labels:\n",
            "  Input:  ['The', 'capital', 'is']  ‚Üê All tokens except the last\n",
            "  Target: ['capital', 'is', 'Paris']   ‚Üê All tokens except the first\n",
            "\n",
            "  Input and target are the SAME tokens, just shifted by 1 position!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Example: Let's say we have the sentence \"The capital is Paris\"\n",
        "sentence = \"The capital is Paris\"\n",
        "tokens = [\"The\", \"capital\", \"is\", \"Paris\"]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"LABEL SHIFTING EXPLAINED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nOriginal sentence: '{sentence}'\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "# Here's what the model sees vs what it should predict:\n",
        "print(\"\\nAt each position, the model sees some tokens and predicts the NEXT token:\\n\")\n",
        "\n",
        "for i in range(len(tokens) - 1):\n",
        "    input_tokens = tokens[:i+1]\n",
        "    target_token = tokens[i+1]\n",
        "    print(f\"Position {i}: Model sees: {input_tokens}\")\n",
        "    print(f\"           Model should predict: '{target_token}'\")\n",
        "    print()\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(\"\\nThis is why we SHIFT the labels:\")\n",
        "print(f\"  Input:  {tokens[:-1]}  ‚Üê All tokens except the last\")\n",
        "print(f\"  Target: {tokens[1:]}   ‚Üê All tokens except the first\")\n",
        "print(\"\\n  Input and target are the SAME tokens, just shifted by 1 position!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Do We Shift?\n",
        "\n",
        "**Simple explanation:** Language models predict the **next** token, not the current one.\n",
        "\n",
        "**Visual representation:**\n",
        "```\n",
        "Sentence: \"The capital is Paris\"\n",
        "\n",
        "Input sequence:  [\"The\",  \"capital\",  \"is\",  \"Paris\"]\n",
        "                    ‚Üì         ‚Üì        ‚Üì       ‚Üì\n",
        "Target sequence: [\"capital\", \"is\",   \"Paris\", ...]\n",
        "                 (predict)  (predict) (predict)\n",
        "```\n",
        "\n",
        "When we give the model `\"The\"`, we want it to predict `\"capital\"` (the next token).  \n",
        "When we give the model `\"The capital\"`, we want it to predict `\"is\"` (the next token).  \n",
        "And so on...\n",
        "\n",
        "**In code, this looks like:**\n",
        "```python\n",
        "# Original tokens: [0, 1, 2, 3, 4, 5]\n",
        "input_ids = tokens[:-1]   # [0, 1, 2, 3, 4]  ‚Üê All except last\n",
        "labels = tokens[1:]        # [1, 2, 3, 4, 5]  ‚Üê All except first\n",
        "```\n",
        "\n",
        "The model learns by:\n",
        "1. Taking `input_ids[i]` (e.g., token 0, 1, 2)\n",
        "2. Predicting what comes next\n",
        "3. Checking against `labels[i]` (e.g., token 1, 2, 3)\n",
        "4. Computing the loss\n",
        "5. Updating weights to get better at prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Concrete Example with Token IDs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LABEL SHIFTING WITH REAL TOKEN IDs\n",
            "================================================================================\n",
            "\n",
            "Original text: 'The capital is Paris'\n",
            "\n",
            "Token IDs:      [464, 3139, 318, 6342]\n",
            "Token strings:  ['The', ' capital', ' is', ' Paris']\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SHIFTING:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Input IDs:  [464, 3139, 318]  ‚Üê Remove last token\n",
            "Labels:     [3139, 318, 6342]     ‚Üê Remove first token\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TRAINING PAIRS:\n",
            "--------------------------------------------------------------------------------\n",
            "Position 0: Input token = 'The' (ID: 464)\n",
            "           Target to predict = ' capital' (ID: 3139)\n",
            "\n",
            "Position 1: Input token = ' capital' (ID: 3139)\n",
            "           Target to predict = ' is' (ID: 318)\n",
            "\n",
            "Position 2: Input token = ' is' (ID: 318)\n",
            "           Target to predict = ' Paris' (ID: 6342)\n",
            "\n",
            "================================================================================\n",
            "üí° Key Point: Same sequence, just shifted by 1 position!\n",
            "   This is called 'next-token prediction' or 'autoregressive training'\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Let's use actual token IDs to make this crystal clear\n",
        "text = \"The capital is Paris\"\n",
        "\n",
        "# Tokenize the text\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "token_ids = tokenizer.encode(text)\n",
        "token_strings = [tokenizer.decode([tid]) for tid in token_ids]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"LABEL SHIFTING WITH REAL TOKEN IDs\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nOriginal text: '{text}'\")\n",
        "print(f\"\\nToken IDs:      {token_ids}\")\n",
        "print(f\"Token strings:  {token_strings}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SHIFTING:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Create input_ids and labels (shifted by 1)\n",
        "input_ids = token_ids[:-1]  # All except last\n",
        "labels = token_ids[1:]       # All except first\n",
        "\n",
        "print(f\"\\nInput IDs:  {input_ids}  ‚Üê Remove last token\")\n",
        "print(f\"Labels:     {labels}     ‚Üê Remove first token\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TRAINING PAIRS:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for i in range(len(input_ids)):\n",
        "    input_token = token_strings[i]\n",
        "    target_token = token_strings[i+1]\n",
        "    print(f\"Position {i}: Input token = '{input_token}' (ID: {input_ids[i]})\")\n",
        "    print(f\"           Target to predict = '{target_token}' (ID: {labels[i]})\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üí° Key Point: Same sequence, just shifted by 1 position!\")\n",
        "print(\"   This is called 'next-token prediction' or 'autoregressive training'\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connecting Label Shifting to Loss Calculation\n",
        "\n",
        "Now you can see how everything fits together:\n",
        "\n",
        "1. **We shift the labels** so each input predicts the next token\n",
        "2. **The model outputs probabilities** for all possible next tokens (the entire vocabulary)\n",
        "3. **We compute cross-entropy loss** by comparing:\n",
        "   - Model's predicted probability distribution\n",
        "   - The actual next token (from our shifted labels)\n",
        "4. **Lower loss** means the model is good at predicting what comes next\n",
        "5. **Training minimizes this loss** across millions of examples\n",
        "\n",
        "**Example in practice:**\n",
        "```python\n",
        "# Input:  \"The\"       ‚Üí Model predicts probabilities for next token\n",
        "# Label:  \"capital\"   ‚Üí We check: did it assign high probability to \"capital\"?\n",
        "# Loss:   -log(P(\"capital\")) ‚Üí If yes, low loss. If no, high loss!\n",
        "```\n",
        "\n",
        "This simple idea of **shifted labels** + **cross-entropy loss** is how all language models learn, from GPT to Claude to Llama!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
