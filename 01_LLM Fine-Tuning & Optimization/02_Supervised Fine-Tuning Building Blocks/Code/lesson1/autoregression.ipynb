{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”„ Autoregression\n",
        "\n",
        "In this notebook, we'll explore:\n",
        "1. **Autoregression** - How models generate text by feeding outputs back as inputs\n",
        "2. **Step-by-Step Generation** - Visualize the predict â†’ append â†’ predict loop\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ Setup: Load Model and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (2.9.0)\n",
            "Requirement already satisfied: transformers in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (4.57.1)\n",
            "Requirement already satisfied: matplotlib in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (3.10.7)\n",
            "Requirement already satisfied: plotly in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (6.3.1)\n",
            "Requirement already satisfied: ipywidgets in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (8.1.7)\n",
            "Requirement already satisfied: python-dotenv in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (1.2.1)\n",
            "Requirement already satisfied: filelock in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from transformers) (2.3.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from transformers) (2025.10.23)\n",
            "Requirement already satisfied: requests in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from matplotlib) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from plotly) (2.9.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipywidgets) (9.6.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: decorator in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: wcwidth in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: executing>=1.2.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.14/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (if needed)\n",
        "!pip install torch transformers matplotlib plotly ipywidgets python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "from typing import List, Tuple\n",
        "import warnings\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "login(token=os.getenv(\"HF_TOKEN\"))\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ¤– Load Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¤– Loading Language Model...\n",
            "==================================================\n",
            "Model: meta-llama/Llama-3.2-1B\n",
            "Model loaded successfully!\n",
            "Vocabulary size: 128,256 tokens\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ¤– Loading Language Model...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "print(f\"Model: {model_name}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.eval()\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Vocabulary size: {vocab_size:,} tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: Understanding Autoregression\n",
        "\n",
        "## ðŸ”„ What is Autoregression?\n",
        "\n",
        "**Autoregression** means the model uses its own previous outputs as inputs for the next prediction.\n",
        "\n",
        "Think of it as a **self-feeding loop**:\n",
        "\n",
        "```\n",
        "Step 1: \"The cat\" â†’ predict \"sat\" â†’ context becomes \"The cat sat\"\n",
        "Step 2: \"The cat sat\" â†’ predict \"on\" â†’ context becomes \"The cat sat on\"\n",
        "Step 3: \"The cat sat on\" â†’ predict \"the\" â†’ context becomes \"The cat sat on the\"\n",
        "Step 4: \"The cat sat on the\" â†’ predict \"mat\" â†’ DONE!\n",
        "```\n",
        "\n",
        "**Key Insight:** The model NEVER plans ahead. It only predicts ONE token at a time based on everything before it.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_next_token_probabilities(\n",
        "    context: str, \n",
        "    top_k: int = 10\n",
        ") -> Tuple[List[str], List[float], torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Get next token probabilities with temperature scaling.\n",
        "    \n",
        "    Args:\n",
        "        context: Input text\n",
        "        top_k: Number of top tokens to return\n",
        "    \n",
        "    Returns:\n",
        "        tokens: List of top-k token strings\n",
        "        probabilities: List of probabilities for those tokens\n",
        "        full_probs: Full probability distribution (for analysis)\n",
        "    \"\"\"\n",
        "    # Tokenize input\n",
        "    input_ids = tokenizer.encode(context, return_tensors='pt')\n",
        "    \n",
        "    # Get model predictions (logits)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits[0, -1, :]  # Last token's logits\n",
        "    \n",
        "    \n",
        "    # Convert to probabilities\n",
        "    probs = F.softmax(logits, dim=0)\n",
        "    \n",
        "    # Get top-k tokens\n",
        "    top_probs, top_indices = torch.topk(probs, top_k)\n",
        "    \n",
        "    # Convert to readable tokens\n",
        "    tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
        "    probabilities = top_probs.cpu().numpy().tolist()\n",
        "    \n",
        "    return tokens, probabilities, probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Visualize Step-by-Step Autoregression\n",
        "\n",
        "Let's watch the model generate text **token by token** and see the probabilities at each step!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_autoregressive_generation(\n",
        "    initial_context: str,\n",
        "    num_steps: int = 5,\n",
        "    top_k: int = 10\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize step-by-step autoregressive generation.\n",
        "    Shows how context grows and probabilities shift at each step.\n",
        "    \"\"\"\n",
        "    context = initial_context\n",
        "    \n",
        "    print(\"ðŸ”„ AUTOREGRESSIVE GENERATION\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Starting Context: \\\"{context}\\\"\")\n",
        "    print(f\"Generating {num_steps} tokens...\\\\n\")\n",
        "    \n",
        "    for step in range(1, num_steps + 1):\n",
        "        print(\"â”€\" * 80)\n",
        "        print(f\"STEP {step}\")\n",
        "        print(\"â”€\" * 80)\n",
        "        \n",
        "        # Get probabilities\n",
        "        tokens, probs, _ = get_next_token_probabilities(\n",
        "            context, top_k=top_k\n",
        "        )\n",
        "        \n",
        "        # Pick the top token (greedy)\n",
        "        next_token = tokens[0]\n",
        "        next_prob = probs[0]\n",
        "        \n",
        "        # Show current context\n",
        "        print(f\"Current Context: \\\"{context}\\\"\")\n",
        "        print(f\"\\\\nTop {top_k} Next Token Predictions:\")\n",
        "        \n",
        "        for i, (token, prob) in enumerate(zip(tokens[:5], probs[:5]), 1):\n",
        "            bar = 'â–ˆ' * int(prob * 50)\n",
        "            print(f\"  {i}. \\\"{token}\\\" {bar} {prob:.4f} ({prob*100:.2f}%)\")\n",
        "        \n",
        "        # Select and append\n",
        "        print(f\"\\\\nSELECTED: \\\"{next_token}\\\" (probability: {next_prob:.4f})\")\n",
        "        \n",
        "        # Update context (autoregression!)\n",
        "        context = context + next_token\n",
        "        \n",
        "        print(f\"ðŸ”„ NEW Context: \\\"{context}\\\"\")\n",
        "        print()\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"GENERATION COMPLETE!\")\n",
        "    print(f\"\\\\nFinal Output: \\\"{context}\\\"\")\n",
        "    print(\"\\\\nNotice: Each prediction used ALL previous tokens as context!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ§ª Demo: Watch Autoregression in Action!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ AUTOREGRESSIVE GENERATION\n",
            "================================================================================\n",
            "Starting Context: \"The cat sat\"\n",
            "Generating 4 tokens...\\n\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "STEP 1\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Current Context: \"The cat sat\"\n",
            "\\nTop 10 Next Token Predictions:\n",
            "  1. \" on\" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.7178 (71.78%)\n",
            "  2. \" in\" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.1100 (11.00%)\n",
            "  3. \" down\" â–ˆ 0.0275 (2.75%)\n",
            "  4. \" up\"  0.0179 (1.79%)\n",
            "  5. \" next\"  0.0137 (1.37%)\n",
            "\\nSELECTED: \" on\" (probability: 0.7178)\n",
            "ðŸ”„ NEW Context: \"The cat sat on\"\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "STEP 2\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Current Context: \"The cat sat on\"\n",
            "\\nTop 10 Next Token Predictions:\n",
            "  1. \" the\" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.7368 (73.68%)\n",
            "  2. \" my\" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.1199 (11.99%)\n",
            "  3. \" a\" â–ˆâ–ˆ 0.0456 (4.56%)\n",
            "  4. \" your\"  0.0191 (1.91%)\n",
            "  5. \" top\"  0.0167 (1.67%)\n",
            "\\nSELECTED: \" the\" (probability: 0.7368)\n",
            "ðŸ”„ NEW Context: \"The cat sat on the\"\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "STEP 3\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Current Context: \"The cat sat on the\"\n",
            "\\nTop 10 Next Token Predictions:\n",
            "  1. \" mat\" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.7249 (72.49%)\n",
            "  2. \" table\"  0.0157 (1.57%)\n",
            "  3. \" lap\"  0.0141 (1.41%)\n",
            "  4. \" hat\"  0.0135 (1.35%)\n",
            "  5. \" hot\"  0.0116 (1.16%)\n",
            "\\nSELECTED: \" mat\" (probability: 0.7249)\n",
            "ðŸ”„ NEW Context: \"The cat sat on the mat\"\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "STEP 4\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Current Context: \"The cat sat on the mat\"\n",
            "\\nTop 10 Next Token Predictions:\n",
            "  1. \".\n",
            "\" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.1556 (15.56%)\n",
            "  2. \",\" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.1343 (13.43%)\n",
            "  3. \".\" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.1313 (13.13%)\n",
            "  4. \"\n",
            "\" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.1098 (10.98%)\n",
            "  5. \" and\" â–ˆâ–ˆâ–ˆâ–ˆ 0.0990 (9.90%)\n",
            "\\nSELECTED: \".\n",
            "\" (probability: 0.1556)\n",
            "ðŸ”„ NEW Context: \"The cat sat on the mat.\n",
            "\"\n",
            "\n",
            "================================================================================\n",
            "GENERATION COMPLETE!\n",
            "\\nFinal Output: \"The cat sat on the mat.\n",
            "\"\n",
            "\\nNotice: Each prediction used ALL previous tokens as context!\n"
          ]
        }
      ],
      "source": [
        "# Demo 1: Generate from \"The cat sat\"\n",
        "visualize_autoregressive_generation(\n",
        "    initial_context=\"The cat sat\",\n",
        "    num_steps=4,\n",
        "    top_k=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸŽ¬ Summary: Key Takeaways from Video 2\n",
        "\n",
        "\n",
        "\n",
        "## ðŸ”„ Autoregression\n",
        "\n",
        "![auto-regression](auto-regressive.png)\n",
        "\n",
        "\n",
        "1. **Self-Feeding Loop**: Each output becomes the next input\n",
        "2. **No Planning Ahead**: Model only predicts ONE token at a time\n",
        "3. **Context Grows**: Every prediction adds to the context window\n",
        "4. **Fragile Process**: One wrong token can derail the entire generation\n",
        "\n",
        "\n",
        "## ðŸ’¡ Connection to Fine-Tuning\n",
        "\n",
        "- Fine-tuning doesn't change HOW the model generates (still autoregressive)\n",
        "- It changes WHAT patterns the model follows\n",
        "- Temperature is a generation-time control you can use with any model\n",
        "- Understanding autoregression helps you understand why context matters so much in fine-tuning data!\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
