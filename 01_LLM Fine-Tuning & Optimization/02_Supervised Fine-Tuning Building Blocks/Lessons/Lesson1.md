# LLM Fine-Tuning Foundations: Understanding Next-Token Prediction

Last week, you explored the landscape of large language models â€” architectures, ecosystems, and when to fine-tune versus use RAG.
Now weâ€™re shifting gears to focus on something more fundamental: how fine-tuning actually works.

To understand that, we first need to zoom in on how language models learn in the first place.
At the core of that learning process is a single, powerful idea: next-token prediction.

Every training step, every fine-tuning run, every loss curve youâ€™ll see later â€” it all traces back to this one mechanism.

Thatâ€™s the key insight here: language models arenâ€™t mysterious text generators â€” theyâ€™re enormous classifiers trained to predict what comes next, choosing from tens of thousands of possible tokens at every step.

And once you see them that way, everything about fine-tuning starts to make sense.

## The Big Idea: Predicting What Comes Next
Letâ€™s start with something familiar.
If youâ€™ve ever used predictive text on your phone, youâ€™ve already seen a miniature version of what language models do.

Type â€œThe cat sat on theâ€¦â€ and your phone might suggest â€œmat.â€
Simple enough. But behind that tiny prediction lies the same principle that powers billion-parameter models.

Hereâ€™s whatâ€™s really happening: when the model predicts â€œmat,â€ it isnâ€™t choosing from just a few options.
It actually computes probabilities for every single token in its vocabulary â€” words, subwords, symbols, and even awkward or irrelevant ones like â€œmitochondrion,â€ â€œchemistry,â€ or â€œwith.â€

Considering that a typical language modelâ€™s vocabulary can include around 50,000 tokens, each prediction involves scoring all of them before picking the next one.

Letâ€™s visualize that:

| Candidate Token | Probability |
| :--- | :--- |
| mat | 0.78 |
| floor | 0.12 |
| chair | 0.05 |
| bed | 0.02 |
| table | 0.01 |

The model doesnâ€™t know the answer. It computes probabilities for every possible token â€” even nonsensical ones â€” and then chooses one, either by selecting the highest probability or sampling from the distribution.

That single classification decision â€” which token should come next â€” is the core of everything that follows in fine-tuning and beyond.

### Video Walkthrough: How Language Models Predict the Next Token
In this video, youâ€™ll see how large language models convert text generation into a massive classification problem â€” predicting the next token from tens of thousands of possibilities at every step.

## From Classification to Generation: The Autoregressive Loop
So far, weâ€™ve focused on how a model makes a single prediction.
Now letâ€™s connect that to how it generates entire paragraphs or conversations.

Language models are autoregressive, meaning they use their own previous outputs as inputs for the next prediction.
Think of it as a self-feeding loop.

Letâ€™s walk through an example step by step:

**User**: Whatâ€™s the capital of France?
**Assistant**:
1. The model predicts â€œTheâ€ as the next token.
2. Then it feeds â€œTheâ€ back in and predicts â€œcapital.â€
3. Now it sees â€œThe capitalâ€ and predicts â€œof.â€
4. Then â€œFrance.â€
5. Then â€œis.â€
6. Then â€œParis.â€

Each step builds on everything before it â€” just like you might build a sentence in your head word by word.
But remember: the model isnâ€™t planning ahead. Itâ€™s only ever predicting one token at a time, given the full context so far.

Thatâ€™s why this process is so powerful and so fragile.
Every new token depends entirely on the history it just created.
One wrong prediction can change the entire trajectory of the response.

ğŸ¥ **How Autoregression Works in Large Language Models**
In this video, youâ€™ll see how large language models generate text by feeding their own predictions back as input â€” turning simple nextâ€‘token prediction into full sentence generation.

## Why Model Outputs Vary: Probabilistic Sampling
If youâ€™ve ever asked ChatGPT the same question twice and got slightly different answers, youâ€™ve seen probabilistic sampling in action.

When generating text, the model doesnâ€™t always pick the most likely token.
Instead, it samples from the distribution to introduce controlled randomness â€” the same way a human writer might choose â€œquick,â€ â€œrapid,â€ or â€œswiftâ€ depending on tone.

This is controlled through parameters like:

* **Temperature** â€” how â€œboldâ€ the sampling is. Low temperature makes the model predictable; high temperature makes it creative.
* **Top-k and Top-p (nucleus) sampling** â€” methods for choosing from the top portion of likely tokens while ignoring unlikely ones.

This combination explains why the same prompt can yield new phrasing each time.
Itâ€™s not inconsistency â€” itâ€™s intentional variety built into the decoding process.

Later in the program, weâ€™ll revisit these settings when we explore inference and generation control.

## Pattern Matching, Not Reasoning
Hereâ€™s the trap many people fall into:
because language models sound like theyâ€™re reasoning, itâ€™s easy to forget theyâ€™re not.

They donâ€™t â€œthink.â€ They match patterns.

When a model writes an explanation, it isnâ€™t reasoning through logic.
Itâ€™s predicting what text usually follows similar text it has seen before.

For example, if itâ€™s seen millions of examples of people solving math problems step by step, itâ€™s learned that pattern. So when prompted, it generates â€œreasoning-likeâ€ sequences â€” but without real understanding.

This is crucial for fine-tuning.
Youâ€™re not teaching the model new reasoning abilities â€” youâ€™re shaping which patterns it reproduces.

## How Fine-Tuning Builds on This
Now that weâ€™ve seen how models predict and generate, letâ€™s connect this back to fine-tuning.

Fine-tuning doesnâ€™t alter the learning algorithm.
It doesnâ€™t teach the model how to learn â€” it changes what it learns from.

Hereâ€™s what shifts:

* **The data format**: from raw text to structured examples like instructions or dialogues.
* **The masking strategy**: deciding which parts of the text actually contribute to loss (weâ€™ll cover this soon).
* **The scale**: from trillions of pretraining tokens to thousands or millions of curated fine-tuning examples.

The mechanism â€” predicting the next token â€” stays exactly the same.
Youâ€™re simply giving the model new kinds of patterns to learn from.

Think of it like a chef who already knows how to cook â€” fine-tuning doesnâ€™t teach them to hold a knife again; it teaches them a new cuisine.

Understanding this foundation sets up everything that comes next.

Youâ€™ll often hear the terms training, pretraining, and fine-tuning used as if they describe different processes â€” but technically, theyâ€™re the same.

In all cases, the model updates its weights by comparing predicted tokens to the correct ones and minimizing the loss.
The only real difference lies in what data and what purpose the training serves:

* **Pretraining**: The model learns general language patterns from a massive, diverse corpus.
* **Fine-tuning**: We continue the same process, but on a smaller, specialized dataset â€” for example, medical notes, legal documents, or chat-style conversations.

In other words, â€œfine-tuningâ€ isnâ€™t a new algorithm â€” itâ€™s just training again, with a different goal and a more focused dataset.
We use the term â€œfine-tuningâ€ because it captures how we use the process: to refine a broad model for a narrower purpose.
